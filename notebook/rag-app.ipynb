{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be using a pdf file which contains the information about the Series of Llama models. The pdf file is accumulated from the wikipedia page of Llama. The pdf file is available in the GitHub repository of this blog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def create_chunks_from_pdf(data_path, chunk_size, chunk_overlap):\n",
    "\n",
    "   '''\n",
    "   This function takes a directory of PDF files and creates chunks of text from each file.\n",
    "   The text is split into chunks of size `chunk_size` with an overlap of `chunk_overlap`.\n",
    "   This chunk is then converted into a langchain Document object.\n",
    "\n",
    "   Args:\n",
    "      data_path (str): The path to the directory containing the PDF files.\n",
    "      chunk_size (int): The size of each chunk.\n",
    "      chunk_overlap (int): The overlap between each chunk.\n",
    "\n",
    "   Returns:\n",
    "      docs (list): A list of langchain Document objects, each containing a chunk of text.\n",
    "   '''\n",
    "\n",
    "   # Load the documents from the directory\n",
    "   loader = DirectoryLoader(data_path, loader_cls=PyPDFLoader)\n",
    "\n",
    "   # Split the documents into chunks\n",
    "   text_splitter = RecursiveCharacterTextSplitter(\n",
    "      chunk_size=chunk_size,\n",
    "      chunk_overlap=chunk_overlap,\n",
    "      length_function=len,\n",
    "      is_separator_regex=False,\n",
    "   )\n",
    "   docs = loader.load_and_split(text_splitter=text_splitter)\n",
    "   return docs\n",
    "\n",
    "data_path = '../data'\n",
    "chunk_size = 500\n",
    "chunk_overlap = 50\n",
    "\n",
    "docs = create_chunks_from_pdf(data_path, chunk_size, chunk_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first start by loading the documents from the directory. We then split the documents into several chunks of equal size and finally convert them to langchain docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the next step is to index the documents in the Qdrant database. But before moving on to that step, we will first need to load the embeddings model which we will be using to convert the documents into embeddings. To build the simple RAG model we will first start with 'BAAI/bge-large-en' embeddings model. In the evaluation section, we will also try with other embeddings models to see how the performance changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deepkapha/anaconda3/envs/ai-chatbot/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_models = ['BAAI/bge-large-en']\n",
    "\n",
    "# Load the embeddings model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_models[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, Now that we have created the chunked documents, loaded the embeddings model, we can now index the documents using Qdrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import Qdrant\n",
    "\n",
    "def index_documents_and_retrieve(docs, embeddings):\n",
    "\n",
    "    '''\n",
    "    This function uses the Qdrant library to index the documents using the chunked text and embeddings model.\n",
    "    For the simplicity of the example, we are using in-memory storage only.\n",
    "\n",
    "    Args:\n",
    "    docs: List of documents generated from the document loader of langchain\n",
    "    embeddings: List of embeddings generated from the embeddings model\n",
    "\n",
    "    Returns:\n",
    "    retriever: Qdrant retriever object which can be used to retrieve the relevant documents\n",
    "    '''\n",
    "\n",
    "    qdrant = Qdrant.from_documents(\n",
    "        docs,\n",
    "        embeddings,\n",
    "        location=\":memory:\",  # Local mode with in-memory storage only\n",
    "        collection_name=\"my_documents\",\n",
    "    )\n",
    "\n",
    "    retriever = qdrant.as_retriever()\n",
    "\n",
    "    return retriever\n",
    "\n",
    "retriever = index_documents_and_retrieve(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the simplicity of this blog, we have indexed the documents in the Qdrant database using the in-memory mode. But in the production environment, you can use the persistent mode to store the indexed documents in the disk. If you want to know more about these approaches you can visit my other blogs on Qdrant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the retriever object, we can use it to retrieve the relevant documents based on the query and finally generate the answer using the LLM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "model_id = \"llama3:instruct\"\n",
    "\n",
    "# Load the Llama-3 model using the Ollama\n",
    "llm = ChatOllama(model=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the simple RAG chain from the retriever and the LLM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "def build_rag_chain(llm, retriever):\n",
    "\n",
    "    '''\n",
    "    This function builds the RAG chain using the LLM model and the retriever object. \n",
    "    The RAG chain is built using the following steps:\n",
    "    1. Retrieve the relevant documents using the retriever object\n",
    "    2. Pass the retrieved documents to the LLM model along with prompt generated using the context and question\n",
    "    3. Parse the output of the LLM model\n",
    "\n",
    "    Args:\n",
    "    llm: LLM model object\n",
    "    retriever: Qdrant retriever object\n",
    "\n",
    "    Returns:\n",
    "    rag_chain: RAG chain object which can be used to answer the questions based on the context\n",
    "    '''\n",
    "    \n",
    "    template = \"\"\"\n",
    "        Answer the question based only on the following context:\n",
    "        \n",
    "        {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"context\",\"question\"]\n",
    "        )\n",
    "    \n",
    "    rag_chain = (\n",
    "        {\"context\": retriever,  \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return rag_chain\n",
    "\n",
    "rag_chain = build_rag_chain(llm, retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the RAG model, we can use it to generate the answers for the queries. Let's test it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, this document appears to be about LLaMA, a language model that was initially announced in February 2023. The document discusses the release of the model, its training and architecture, as well as reactions to its leak and accessibility. It also mentions issues related to unauthorized distribution, DMCA takedown requests, and subsequent releases of updated versions under different licenses.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke('What is this document about?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you go! You have successfully built a RAG App using Langchain, Qdrant, HuggingFace, Ollama and Llama-3. You can now use this RAG App to answer questions based on the context of the documents.\n",
    "\n",
    "Now let's move on to the next section where we will learn how to evaluate the RAG App using the evaluation metrics discussed in the above sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
